# Zookeepr Session

    Sesions offer order guarantees, which mean that requests in a session are 
  executed in FIFO order. Typically, a client has only a single session open, so
  its request are all executed in FIFO order. If a client has multiple concurrent
  sessions, FIFO ordering is not necessarily perserved across the sessions.
  Consecutive sessions of the same client, even if they don't overlap in time,
  also do not necessarily preserve FIFO order. Here is how it can happen in this
  case:
    1. Client establishs a session and makes two consecutive asynchronous calls
    to create /tasks and /workers.
    2. First session expires.
    3. Client establishes another session and makes an asynchronous call to 
    create /assign.
    In this sequence of calls, it is possible that only /tasks and /assign have
   been created, which preserves FIFO ordering for the first session but violates
   it across sessions.
   
# Implementing a primitive : locks with ZooKeeper

    ONe simple example of what we can do with ZooKeeper is implement critical 
  sections through locks. There care multiple flavors of locks(e.g., read/write
  locks, global locks) and serveral ways to implement locks with ZooKeeper. Here
  we discuss a simple recipe just to illustrate how applications can use ZooKeeper;
  we do not consider other variants of locks.
    Say that we have an application with n Processes trying to acquire a lock.
  Recall that ZooKeeper does not expose primitives directly, so we need to use 
  the ZooKeeper interface to manipulate zondes and implement the lock. To acquire
  a lock,each process p tries to create a znode, say /lock. If p succeeds in 
  creating the znode, it has the lock and can proceed to execute its critical 
  section. One potential problem is that p could crash and never release the lock
  In this case, no other process will ever be able to acquire the lock again, and
  the system could seize up in a deadlock. To avoid such situation, we just have
  to make the /lock znode ephemeral when we create it.
    Other process that try to create /lock fail so long as the znode exists. So,
  they watch for changes to /lock and try to acquire the lock again once they 
  detect that /lock has been deleted. Upon receiving a notificaiton that /lock 
  has been deleted, if a process p' is still interested in acquiring the lock, 
  it repeats the steps of attempting to create /lock and, if another process has
  created the zonde already,watching it.
  
# ZooKeeper Manages Connections
  
   Don't try to manage ZooKeeper client connections yourself. The ZooKeeper 
 client library monitors the connecton to the service and not only tells your 
 about connection problems, But actively tries to reestablish communication. 
   Often the library can quickly reestablish the session with minimal disruption
 to your application. Needlessly closing and starting a new session just causes
 more load on the system and longer availability outages.
   Of course, the client will continually extend the expiration time as long as 
 it keeps an active connection to a ZooKeeper server.
   Once close is called, the session represented by the ZooKeeper object gets 
 destoryed.
 
# ConnectionLossException

    The ConnectionLossException occurs when a client becomes disconnected from a
  ZooKeeper server. This is usually due to a network error, such as a network 
  partition, or the failure of a ZooKeeper server. When this exception occurs, it
  is unknown to the client whether the request was lost before the ZooKeeper 
  servers processed it, or if they processed it but the client did not receive 
  the response. As we described earlier, the ZooKeeper client library will 
  reestablish the connection for future request, but the process must figure out
  whether a pending request has been processed or whether it should reissue the 
  request.
  
# InterruptedException

    The InterruptedException is caused by a client thread calling Thread.interrupt.
  This is often part of applicaiton shutdown, but it may also be used in other 
  application-dependent ways. This exception literally interrupts the local client
  request processing in the process and leaves the request in an unknown state.
    Unfortunately, in Java there aren't clear guidelines for how to deal with 
  thread interruption, or even what it means. Sometimes the interruptions are 
  used to signal threads that things are being shutdown and they need to clean up.
  In other cases, an interruption is used to get control of a thread, but execut-
  ion of the applicaition continues.
    Handling of InterruptedExcetion depends on our context. If the interrupteEx-
  pection will bubble up and eventually close our zk handle, we can let it go up
  the stack and everything will get cleaned up when the handle is closed. If the
  zk handle is not closed, we need to figure out if we are the master before re-
  throwing the exception or asynchronously continuing the operation. This latter 
  cause is particularly tricky and requires careful design to handle properly.
  
 # ZooKeeper Asynchronously request / callback
 
     The asynchronous method simpley queues the request to the ZooKeeper server.
   Transmission happens on another thread. When responses are recevied, they are
   processed on a dedicated callback thread. To preserve order, there a single 
   callback thread and responses are processed in the order they are recevied.
     Because a single thread process all callbacks, if a callback block, it blocks
   all callbacks that follow it. This means that generally you should not do 
   intensive operations or blocking operations in a callback. There may be times
   when it's legitimate to use the synchronous API in a callback, but it should
   generaly be avoided so that subsequent callbacks can be processed quickly.
   
# One-Time Triggers
      A watch is a one-time trigger associated with a znode and a type of event(e.g.
    , data is set in the znode, or the znode is deleted). When the watch the watch
    is triggered by an event, it generates a nitification. A notification is a 
    message to the application client that registered the watch to inform this 
    client of the event.
      When an application process regiesters a watch to recevie a notification,
    the watch is triggered at most once and upon the first event that matches the 
    condition of the watch. For example, say that the client needs to know when a 
    given znode /z is deleted(e.g., a backup master). The client executes an ex-
    ists operation on /z with the watch flag set and waits for the notification.
    The notification comes in the form of a callback to the application client.
      Each watch is associated with the session in which the client sets it. If
    the session expires, pending watches are removed. Watches do, however, persist
    across connections to different servers. Say that a ZooKeeper client disconnects
    from a ZooKeeper server and connects to a different server in the ensemble.
    The client will send a list outstanding watches. When reregiestering the 
    watch, the server will check to see if the watched znode has changed since
    the watch was regiested. If the znode has changed, a watch event will be sent
    to the client; otherwise, the watch will be reregiestered at the new server.
   
# Miss Events with One-time triggers

    An application can miss events between receiving a notification and register-
    ing for another watch. However, this issue deserves more discussion. Missing
    events is typically not a problem because any changes that have occurred dur-
    ing the period between receiving a notification and regiestering a new watch
    can be captured by reading the state of ZooKeeper directly.
    
# Watch Interface

    public void process(WatchEvent event);
    
    - The state of the ZooKeeper session(KeeperState):
      ```
        Disconnected
        SyncConnected
        AuthFailed
        ConnectedReadOnly
        SaslAuthenticated
        Expired
      ```
      
     - The event type(EventType)
       ```
         NodeCreated
         NodeDeleted
         NodeDataChanged
         NodeChildrenChanged
         None
       ```
       
     - A znode path in the case the watched event is not None
     
     - One important observation about watches is that currently it is not poss-
     ible to remove them once set. The only two to remove a watch are to have it
     triggered or for its session to be cloused or expired. This behavior is li-
     kely to change in future versions, however, because the community has been
     working on it for version 3.5.0.
     
# Multiop

    Multiop was not in the original design of ZooKeeper, but was added in version
  3.4.0. Multiop enables the execution of multiple ZooKeeper operations in a  
  block atomically. The execution is atomic in the sense that either all opera-
  tions in a multiop block succeed or all fail. For example, we can delete a pa-
  rent zonde and its child in a multiop block. The only possible outcomes are 
  that either both operations successd or both fail. It is not possible for the
  parent to be deleted while leaving one of its hcildren around, or vice versa.
    Multiop can simplify our master-worker implementation in at least one plcae.
  When assiging a task, the master in previous examples has created the corres-
  ponding assignment znode and the deleted the task znode under /tasks. if the 
  master crashes before deleting the znode under /tasks, we are left with a task
  in /tasks that has already been assigned. Using multiop, we can create the 
  znode representing the assignment of the task under /assign and delete the 
  zonde representing the task unser /tasks atomically. Using this approach, we
  guarantee that no task znode under /tasks has been already assigned. If a 
  backup takes over the role of master, is is not necessary to disambiguate the
  tasks in /tasks: they are all unassigned.
    Another feature that multiop offers is the possibility of checking the 
  version of a znode to enable operations over multiple zondes that read the state
  of ZooKeeper and write back some data-possibly a modification of what has been
  read. The version of the znode that is checked does not change, so this call
  enables a multiop that checks the version of z znode that is not modified. This
  feature is useful when the changes to one or more znodes are conditional upon 
  the version of another znode. Say that in our master-worker example, the master
  needs to have the clients adding new tasks under a path that the master speifies.
  For example, the master could ask clients to create new task as children of 
  /tasks-mid, where mid is the master identifier. The master stores this path
  as the data of the /master-path znode. A client that needs to add a new task
  first reads /master-pth and picks its current version with Stat. Next, the client
  creates a new task zonde under /task-mid as part of the a multiop call. and it
  also checks that the version of /master-path matches the one it has read.
  
# Recoverable and Unrecoverable Failure

    Rather than trying to determine causes of failures, ZooKeeper expose two cla-
  sses of failure: recoverable and unrecoverable. Recoverable failures are trans-
  ient and should be considered relatively normal-things happen. Brief network 
  hiccups and server failures can cause these kinds of failures. Developers should
  write theire code so that theire applications keep running in spite of these
  failures.
    Unrecoverable failure are much more problematic. These kinds of failures cause
  the ZooKeeper handle to become inoperable. The easiest and most common way to 
  deal with this kind of failure is to exit the application. Examples of causes 
  of this class of failure are session timeouts, networks outages for longer than
  the session timeout, and authentication failures.
  
  
# The Exist Watch and the Disconnected Event

    To make session disconnection and reestablishment a little more seamless, the
  ZooKeeper client library will reestablish any existing watches on the new server.
  When the client library connects to a ZooKeeper server, it will send the list of 
  outstanding watches and the last zxid(the last state tiemstamp) it has been. 
  The server will go through the watches and check the modification timestamps of
  the znodes that correspond to them. If any watched znodes have a modification
  timestamp later than the last zxid seen, the server will trigger the watch.
    This logic works perfectly for every ZooKeeper operation except exist. The
  exists operation is different from all other operatons because it can set a 
  watch on a znode that does not exist. If we look closely at the watch registra-
  tion logic in the previous paragraph, we are that there is a corner case in 
  which we can miss a watch event.
    Figure 5-5 illustrates the corner case that cause us to miss the creation 
   event of a watched znode. In this example, the client is watching for the
   creation of /event. However, just as the /event is created by another client,
   the watching client loses its connection to ZooKeeper. During this time the 
   other client deletes /event, so when the watching client reconnection to Zoo-
   Keeper and reregisters its watch, the ZooKeeper erver no longer has the /event
   znode. Thus, when it processes the regiestered watches and sees the watch for
   /event, and sees that there is no node called /event, it simply reregiesters 
   the watch, causing the client to miss the creation event for /event. Because
   of this corner case, you should try to avoid watching for the creation event
   of a znode. If you do watch for a creation event it should be for a long-lived
   znode; otherwise, this corner case can bite you.
   
   
#  Fencing Token

     Another approach is to extend the coordination data provided by ZooKeeper to
   the external device, using a technique call fencing. This is used often in 
   distributed systems to ensure exclusive access to a resource.
     We will show an example of implementng simple fencing using a fencing token.
   As long as a client holds the most recent token, it can access the resource.
     When we create a leaser znode, we get back a Stat structure. One of the 
   members of that structures it the czxid, which is the zxid that created the 
   znode. The zxid is a unique, monotonically increasing sequence number. We can
   use the czxid as a fencing token.
     When we make a request to the external resource, or when connect to the 
   external resource, we also supply the fencing token. If the external resource
   has recevied a request or connection with a higher fencing token, our request
   or connecton will be rejected. This means that once a new master connects to 
   an external resource and starts managing it, if an older master tries to do 
   anything which the external resource, its request wiil fail; it will be fenced
   off. Fencing has the nice benefit that it will work reliably event in the pre-
   sence of system overload or clock drift. 
     Figure 5-7 shows how this technique solves the scenario of Figure 5-6. When
   c1 becomes the leader at time t1, the creation zxid of /leader znode is 3(in
   reality, the zxid would be a much larger number). It supplies the creation 
   zxid as the fencing token to connect with the database. Later, when c1 becomes
   unresponsive due to overload, ZooKeeper declares c1 as failed and c2 becomes 
   the new leader at time t2. c2 uses 4 as its fencing token because the /leader
   zonde it created has a creation zxid of 4. At time t3, c3 starts making requests
   to the database using its fencing token. Now weh c1's request arrives at the
   datbase at time t4, it is rejected because its fencing token(3) is lower than
   the highest seen fencing tokne(4), thus avoiding corruption.
     Unfortunately, this fencing scheme requires changes to the protocol between
   the client and the resource. There must be room in the protocol to add the 
   zxid, and the external resource needs a persistent store to track the latest
   zxid received.
   
# Order in the presence of connection loss

    Upon a connection loss event, ZooKeeper cancels pending request. For synch-
  ronous cals the library throws an exception, while for asynchronous call the 
  library invokes the callbacks with a return code indicating a connection loss.
  Thie client library won't try to resubmit hte request once it has indicated to
  the applicaiton that the connection has been lost, so it's up to the application
  to resubmit operations that have been canceled. Consequently, the application 
  can repy on the client library to isue al the callbacks, but it cannot rely on
  ZooKeeper to actually carry out the operation in the event of a connection loss.
    To understand what impat this can have on an application, let's consider the
  following sequence of events:
    - Applicaiotn submits a request to execute op1
    - Client library detects a connection loss and cancels pending request to 
      execute op1
    - Client reconnections before the session expires
    - Op2 is executed successfully
    - Op1 returns with CONNETIONLOSS
    - Appliction resubmits op1
     In this case, the application submitted op1 and op2 in that order and got 
  op2 to execute successfully before op1. When the application notices the con-
  nection loss in the callback of op1, it tries to submit that request again. But
  suppose the client does not successfully reconnect. A new call to submit op1
  will report connection loss again, and there is a risk that the application 
  will enter into an unbounded loop of resubmitting the op1 request without con-
  necting. To get out of this cycle, the applicaiton could set a bound on the 
  number of attempts, or close the handle if reconnecting takes too long.
    In some cases, it might be important to make sure that op1 is executed 
  successfully before op2. if op2 depends on op1 in some way, then to avoid hav-
  ing op2 executing successfully before op1, we could simply wait for a success-
  ful execution of op1 before submitting op2. This is the approach we have taken
  in most of our master-worker example code, to guarantee that the request are
  executed in order. In general, this approach of waiting ofr the result of op1
  is safe, but it adds a performance penalty because the application needs to 
  wait for the result of one request to submit the next one, instead of having
  them processed concurrently.

# Order with the synchronous API and multiply thread

    Multithreaded applications are common these days. If you are using the syn-
  chronous API with multiply threads, it is important to pay attention to one
  ordering issue. A synchronous ZooKeeper call blocks until it gets a response.
  If two or more threads submit synchronous operations to ZooKeeper concurrently,
  they will each block until they receive a response. ZooKeeper will deliver the 
  responses in order, but it is possible that due to thread scheduling, the result
  of an operation submitted later will be processed first. If ZooKeeper delivers
  responses to operations very close to each other in time, you may observe such
  a scenario.
    If different threads submit operations concurrently, odds are that the oper-
  ations are not directly related and can be executed in any order without 
  causing consistency issue.
    But if the operations are related, the application client must take the order
  of submission into consideration when procesing the results.    

# Order when mixing synchronous and asychronous calls
    Here's another situation where results may appear to be out of order. Say that
  you submit two asynchronous operation, aop1 and aop2. It doesn't matter what
  the operations are, only that they are asynchronous. In the callback of aop1,
  you make a synchronous call, sop1. The synchronous call blocks the dispatch
  thread of the ZooKeeper client, which causes the application client to recevie
  the result of sop1 before reveiving the result of aop2. Consequently, the 
  application observers the results of aop1, sop1, and aop2 in that order, which
  is different from the submission order.
    In general, it is not a good idea to mix synchronous and asychronous calls.
  There are a few exceptions - for example, when starting up, you may want to 
  make sure that some data is ZooKeeper before proceeding. It is possible to use
  Java latches and order such mechanisms in these case, but one or more 
  synchronous calls might also do the job.
  
# Data and child limits

    ZooKeeper limits the amount of data transferred for eache request to 1MB by
  default. This limit bounds the maximum amount of data for any given node and 
  the number of children any parent znode can have. The choice of 1MB is 
  somewhat arbitrary, in the sense that there is noting fundamental in ZooKeeper
  that prevents it from usng a different value, larger or smaller. A limit 
  exists, however, to keep performance high. A znode with very large data, for
  instance, takes a long time to traverse, essentially stalling the processing
  pipeline while requests are being executed. The same problem happens if a 
  client executes getChildren on a znode with a large number of children.
    The limits ZooKeeper imposes right now for data size and number of children
  are already quite high, so you should avoid event getting close to these 
  limits. We have enabled large limits to satify the requirements of a broader 
  set of applications. If you have a special use case and realyy need different
  limit, you can change them.
  

# Curator framewrok 

    - Sequential znodes
    
      If the server the client is connected to crashes before returning the 
    znode name (with the sequence number) or the client simply disconnections,
    then the client doesn't get a response even if the operation has been 
    executed. As a consequence, the client doesn't know the path to the znode it
    created. Recall that we use sequential znodes, for example, in recipes that
    establish an order for participating clients. To address this problem, 
    CreateBulder provides a withProtection call that tells the Curator client to 
    prefix the sequential znode with a unique identifier. if the create fail, 
    the client retries the operation, and as part of retrying it verifies 
    whether there is a already a znode with the unique identifier.
    
    - Guaranteed deletes
    
      A similar situation occurs with delete operations. If the client
    disconnects from the serve while executing a delete operation, it doesn't
    know whether the delete operation has succeeded or not. If the presence of 
    the znode being deleted indicates, for example, that a resource is locked, 
    it is important to delete the znode to make sure that the resource is free 
    to be used again. The Curator client provides a call that enables an
    application to make the execution of a delete operation guaranteed. The
    operation is guaranteed in the sense that the Curator client reexecutes the
    operation until it succeeds, and for as long as the Curator client instance
    is valid. To use this feature, the DeleteBuilder interface defines a 
    guaranteed call.
    
#  Transactions

     ZooKeeper servers process read request ( exists, getDAta, and getClilden)
   locally. When a server receives, say, a getData request from a client, it 
   reads tis state and returns it to the client. Becase it serves requests 
   locally, ZooKeeper is pretty fast as sreving read-dominiated workloads. We 
   can add more servers to the ZooKeeper ensemble to serve more read requests,
   increading overall throughput capacity.
     Client requests that change the state of ZooKeeper (create, delete, and 
   setData) are forworded to the leader. The leader executes the request, 
   producing a state update that we call transcation. Whereas the request 
   express the operation the way the client originates it, the transaction 
   comprises the steps taken to modify the ZooKeeper state to reflect the 
   execution of the request.
     Say that a client submits a setData request on a given znode /z .setData
   should change the data of the znode and bump up the version number. So, a 
   transaction for this request contains two important field: the new data of 
   the znode and the new version number of the znode. When applying the 
   transaction, a serve simply replaces the adata of /z with the data in the 
   transaction and the version number with the value in the transaction, rather
   than bumping it up.
     A transaction is treated as a unit, in the sense that all changes it 
   contains must be applied atomically. In the setData example, changing the 
   data without an accompanying change to the version accordingly leads to 
   trouble. Consequently, when a Zookeeper ensemble applies transactionn, it 
   makes sure that all changes are applied atomically and there is no 
   interference from other transactions. There is no rollback mechanism like 
   with transactional relational databases. Instead, ZooKeeper makes sure that
   the steps of transactions do not interfere with each other. For a long time,
   the design used a single thread in eache server to apply transactions. 
   Having a single thread guarantees that the transactions are applied 
   sequentially without interference. Recently, ZooKeeper has added support for
   multiple threads to speed up the process of applying transactions.
     A transaction is also idempotent. That is, we can apply the same 
   transaction twice and we will get the same result. We can even apply multiple
   transactions multiple times and get the same result, as long as we apply them
   in the same order every time. We take advantage of this idempotent property
   during recovery.
    When the leader generates a new transaction, it assign to the transaction an
   identifier that we call a ZooKeeper transaction ID (zxid). Zxid identify 
   transactions so that they are applied to the stae of servers in the order 
   established by the leader. Servers also exchange zxid when electing a new 
   leader, so they can determine which nonfaulty server has received more 
   transactions and can sychronize their states.
     A zxid is long(64-bit) integer split into two parts: the epoch and the 
   counter. Each part has 32 bits. The use of epochs and counters will become
   clear when we discuss Zab, the protocol we use to broadcast state updates to
   servers.
   
# Leader Elections

    The leader is server that has been chosen by an ensemble of servers and that
  continues to have support form that ensemble. The purpose of the leader is to
  order client requests that change the ZooKeeper state: create, setData, and 
  delete. The leaser transforms each request into a transactions, as explained 
  in the previous section, and proposes to the followers that the ensemble 
  accepts and applies them in the order issued by the leader.
    To exercise leadership, a server must hava support from a quorum of servers.
  Quorums must intersect to avoid the problem that we call split brain: two 
  subsets of servers making progress independently. This situation leads to 
  inconsistent system state, and clients end up getting different results 
  depending on which server they happen to contact. 
    Each server starts in the LOOKING state, where it must either elect a new 
  leader or find the existing one. If a leaser already exists, other servers
  inform the new one which server is the leaser. At this point, the new server
  connects to the leader and makes sure that its own state is consistent with 
  state of the leader.
    If an ensemble of servers, however, are all in the LOOKING state, they must
  communicate to elect a leader. They exchange messages to converge on a common
  choice for the leader. The server that wins this election enters the LEADING
  state, while the other servers in the ensemble enter the FOLLOWING state.
    The leader election messages are called leader election notifications, or
  notifications. The protocol is extremely simple. When a server enters the 
  LOOKING state, it sends a batch of notification messages, one to each of the
  other servers in the ensemble. The message contains its current vote, which 
  consists of the server's identifier(sid) and the zxid(zxid) of the most recent
  transaction it executed. Thus, (1,5) is a vote sent by the server with a sid 
  of 1 and a most recent zxid of 5. (For the purposes of leader election, a zxid
  is a single number, but in some other protocols it is represented as an epoch
  and a counter.)
    Upon receiving a vote, a server changes its vote according to the following
  rules:
    1. Let voteId and voteZxid be the identifier and the zxid in the current 
  vote of the receiver, whereas myZxid and mySid are the values of the receiver
  itself.
    2. if (voteZxid > myZxid) or (voteZxid = myZxid and voteId > mySid), keep 
  the current vote.
    3. Otherwise, change my vote by assigning myZxid to voteZxid and mySid to 
  vote Zxid.
    In short, the server that is most up to date wins, because it hs the most 
   recent zxid. We'll see later that this simplifies the process of restarting a
   quorum when a leader dies. If multiple servers have the most recent zxid, the
   one with the highest sid wins.
     Once a server recevies the same vote from a quorum of servers, the server 
   declares the leader elected. If the elected leader is the server itself, it
   starts executing the leader role. Otherwise, it becomes a follower and tries
   to connect to the elected leader. Note that it is note guaranteed that the
   follower will be able to connect to the elected leader. The elected leader 
   might have crashed, for example. Once it connects, the follower and the 
   leader sync their state, and only after syncing can the follower start 
   processing new requests.
   
# Zab: Broadcasting State Updates

    Upon receiving a write request, a follower forwards it to the leader. The 
  leader executes the request speculatively and broadcast the result of the 
  execution as a state update, in the form of a transaction. A transaction 
  comprises the exact set of changes that a server must apply to the data tree
  when the transaction is committed. The data tree is the data structure holding
  the ZooKeeper state.
    The next question to answer is how a server determines that a transaction 
  has been committed. This followers a protocol called ZooKeeper Atomic 
  Broadcasting protocol. Assuming that there is an active leader and it has a 
  quorum of followers supporting its leadership, the protocol to commit a 
  transaction is very simple, resembling a two-phrase commit:
    1. The leaser sends a PROPOSAL message, p, to all followers.
    2. Upon receiving p, a follower responds to the leader with an ACK, 
  informing the leader that it has accepted the proposal.
    3. Upon receiving acknowledgements from a quorum(the quorum includes the 
  leader itself), the leader sends a message informing the followers to COMMIT 
  it.
    Before acknowledging a proposal, the follower needs to perform a couople of 
  additional checks. The follower needs to check that the proposal is from the
  the leader it is currently following, and that it is acknowledging proposals 
  and committing transactions in the same order that the leader broadcasts them
  in.
    Zab guarantees a couple of important propertis:
    - If the leader broadcast T and T' in that order, each server must commit T
  before committing T'
    - If any server commits transactions T and T' in that order, all other 
  servers must also commit T before T'.
    The first property guarantees that transactions are delivered in the same
  order across server, whereas the second property guarantees that servers do
  not skip transactions. Given that the transactions are state updates and each
  state update depends upon the previous state update, skipping transactions 
  could create inconsistencies. The two-phrase commit guarantees the ordering of
  transactions. Zab records a transaction in a quorum of server. A quorum must
  acknowledge a transaction before the leader commits it, and a follower records
  on disk the fact that it has acknowledged the transaction.
    Transactions can still end up on some servers and not on others, because 
  servers can fail while trying to write a transaction to storage. ZooKeeper can
  bring all servers up to date whenever a new quorum is created and a new leader
  chosen.
    ZooKeeper, however, does not expect ot have a single active leader the whole
  time. Leaders may crash or become temporarily disconnected, so servers may 
  need to move to a new leader to guarantee that the system remains available.
  The notion of epochs represents the changes in leadership over time. An eposh
  refers to the period during which a given server exercised leadership. During
  an epoch, a leader broadcast proposals and identifies each one according to a
  counter. Remember that each zxid includes the epoch as its first element, so
  each zxid can easily be associated to the epoch in whick the transaction ware
  created.
    The epoch number increase each time a new leader election takes place. This
  same server can be the leader for different epochs, but for the purpose of 
  the protocol, a server exercising leadership in different epochs is perceived 
  as a different leader. If a server s has been the leader of epoch 4 and it
  currently the established leader of epoch 6, a follower following s in epoch 6
  proccesses only the message s sent during epoch6. The follower may accept 
  proposals from epoch 4 during the recovery period of epoch 6, before it starts
  accepting new proposals for epoch 6. Such proposals, however, are sent as part
  of epoch 6's messages.
    Recording accepted proposals in a quorum is critical to ensure that all 
  servers evenetually commit transactions that have been committed by one or 
  more servers, even if the leader crashes. Detecting perfectly that leaders(or
  any server) have crashed it very hard, if not impossible, in many settings, so
  it is very possible to falsely suspect that a leader has crashed.
    Most of the difficulty with implementing a broadcast protocol is related the
  presence of concurrent leaders, not necessarily in a split-brain scenario. 
  Multiple concurrent leaders could make servers commit transactions out of 
  order or skip transaction altogether, whick leave servers with inconsistent
  states. Preventing the system from ever having two servers believing they are
  leaders concurrently is very hard. Timing issues and dropped message might 
  lead to such scenarios, so the broadcast protocal cannot rely on this 
  assumption. To get around this problem, Zab guarantees that:
    - An elected leader has commited all transactinos that will ever be 
   committed from previous epochs before it starts broadcasing new transactions.
    - At no point in time will two servers have a quorum of supporters.
    To implement the first requirement, a leader does not become active until 
  makes sure that a quorum of servers agrees on the state it should start with 
  for the new epoch. The initial state of an epoch must encompass all
  transactions that have been previously committed, and possibly some other ones
  that had been accepted before but not committed. It is important, though, that
  before the leader makes any new proposals for epoch e, it commits all 
  proposals that wille ever be committed from epochs up to and including e-1. if
  there is a proposal lying around from epoch e' < e and it is not commited by 
  the leader of e by the time it make the first proposal of e, the old proposal
  is never committed.
    The second porint is somewhat tricky because it doesn't really prevent two
  leaders from making progress independently. Say that a leader l is leading and
  broadcasting transactions. At some point, a quorum of servers Q believes l is
  gone, and it elects a new leader, l'. let's say that T is a transaction that
  was being broadcast at the tiem Q abandoned l, and that a strict subset of Q
  has successfully record T. After l' is elected, enough process not in Q also
  record T, forming a quorum for T. in this case, T is committed even after l'
  has been elected. But don't worry; this is not a bug. Zab guarantees that T is
  part of the transactions committed by l', by guaranteeing that the quorum of
  supporters of l' contain at least one follower that has acknowledged T.  The 
  key point here is that l' and l do not have a quorum of supporters 
  simultaneously.
    In the figure, l is server s5, l' is server s3, Q comprise s1 through s3, 
  and the zxid(1,1). After receiving the second comfirmation, s5 is able to send
  a commit message to s4 to tell it to commit the transaction. The other servers
  ignore message from s5 once they start following s3. Note that s3 
  acknowledged(1,1), so it si aware of the transaction when it established 
  leadership.
    We have just promised that Zab ensures the new leader l' does not miss(1,1),
  but how does it happen exactly? Before becoming active, the new leader l' must
  learn all proposals that servers in the old quorum have accepted previously,
  and it must get a promise that these servers won't accept further proposals 
  from previous leader. In the example , the servers forming a quorum and 
  supporting l' promise that they won;t accept any more proposals from leader l.
  At that point, if leader l is still able to commit any propotsal, as it does
  with (1,1), the proposal must have been accepted by at least one server in the
  quorum that made the promise to the new leader. Recal that quorums must
  overlap in at least one server, so the quorum that l uses to commit and the 
  quorum that l' talks to must have at least one server in common. Consequently,
  l' includes (1,1) in its state and propagates it to its followers.
    Recall that when electing a leader, servers pick the one with the highest 
  zxid. This saves ZooKeeper from having to transfer proposals from followers to
  the leader; it only needs to transfer state from the leader to the followers.
  Say instead that we have at least one follower that has accepted a proposal 
  that the leader hasn't. Before syncing up with the other followers, the leader
  would have no recevie and accept the proposal. However, if we pick the server
  with the highest zxid, then we can completely skip this step and jump directly
  into bringing the follwers up to date.
    When transitioning between epochs, ZooKeeper uses two different ways to 
  update the followers in order to optimize the process. If the follower is not
  too far behind the leader, the leader simply sends the missing transactions.
  They are always the most recent transactions, because followers accept all
  transactions in strict orfer. This update is called a DIFF in the code. If the
  follower is lagging far behind, ZooKeeper does a full snapshot transfer, 
  called a SNAP in the code. Doing a full snapshot transfer increases recovery
  time, so sending a few missing transactions is preferable, but not always 
  possible if the follower is far behind.
    The DIFF a leader sends to a follower corresponds to the proposals that the
  leader has in its transaction log, whereas the SNAP is the lastest valid 
  snapshot that the leader has. 
   
  
# references
 
  - ZooKeeper Distributed Process Coordination.O'Reilly.2013