# Zookeepr Session

    Sesions offer order guarantees, which mean that requests in a session are 
  executed in FIFO order. Typically, a client has only a single session open, so
  its request are all executed in FIFO order. If a client has multiple concurrent
  sessions, FIFO ordering is not necessarily perserved across the sessions.
  Consecutive sessions of the same client, even if they don't overlap in time,
  also do not necessarily preserve FIFO order. Here is how it can happen in this
  case:
    1. Client establishs a session and makes two consecutive asynchronous calls
    to create /tasks and /workers.
    2. First session expires.
    3. Client establishes another session and makes an asynchronous call to 
    create /assign.
    In this sequence of calls, it is possible that only /tasks and /assign have
   been created, which preserves FIFO ordering for the first session but violates
   it across sessions.
   
# Implementing a primitive : locks with ZooKeeper

    ONe simple example of what we can do with ZooKeeper is implement critical 
  sections through locks. There care multiple flavors of locks(e.g., read/write
  locks, global locks) and serveral ways to implement locks with ZooKeeper. Here
  we discuss a simple recipe just to illustrate how applications can use ZooKeeper;
  we do not consider other variants of locks.
    Say that we have an application with n Processes trying to acquire a lock.
  Recall that ZooKeeper does not expose primitives directly, so we need to use 
  the ZooKeeper interface to manipulate zondes and implement the lock. To acquire
  a lock,each process p tries to create a znode, say /lock. If p succeeds in 
  creating the znode, it has the lock and can proceed to execute its critical 
  section. One potential problem is that p could crash and never release the lock
  In this case, no other process will ever be able to acquire the lock again, and
  the system could seize up in a deadlock. To avoid such situation, we just have
  to make the /lock znode ephemeral when we create it.
    Other process that try to create /lock fail so long as the znode exists. So,
  they watch for changes to /lock and try to acquire the lock again once they 
  detect that /lock has been deleted. Upon receiving a notificaiton that /lock 
  has been deleted, if a process p' is still interested in acquiring the lock, 
  it repeats the steps of attempting to create /lock and, if another process has
  created the zonde already,watching it.
  
# ZooKeeper Manages Connections
  
   Don't try to manage ZooKeeper client connections yourself. The ZooKeeper 
 client library monitors the connecton to the service and not only tells your 
 about connection problems, But actively tries to reestablish communication. 
   Often the library can quickly reestablish the session with minimal disruption
 to your application. Needlessly closing and starting a new session just causes
 more load on the system and longer availability outages.
   Of course, the client will continually extend the expiration time as long as 
 it keeps an active connection to a ZooKeeper server.
   Once close is called, the session represented by the ZooKeeper object gets 
 destoryed.
 
# ConnectionLossException

    The ConnectionLossException occurs when a client becomes disconnected from a
  ZooKeeper server. This is usually due to a network error, such as a network 
  partition, or the failure of a ZooKeeper server. When this exception occurs, it
  is unknown to the client whether the request was lost before the ZooKeeper 
  servers processed it, or if they processed it but the client did not receive 
  the response. As we described earlier, the ZooKeeper client library will 
  reestablish the connection for future request, but the process must figure out
  whether a pending request has been processed or whether it should reissue the 
  request.
  
# InterruptedException

    The InterruptedException is caused by a client thread calling Thread.interrupt.
  This is often part of applicaiton shutdown, but it may also be used in other 
  application-dependent ways. This exception literally interrupts the local client
  request processing in the process and leaves the request in an unknown state.
    Unfortunately, in Java there aren't clear guidelines for how to deal with 
  thread interruption, or even what it means. Sometimes the interruptions are 
  used to signal threads that things are being shutdown and they need to clean up.
  In other cases, an interruption is used to get control of a thread, but execut-
  ion of the applicaition continues.
    Handling of InterruptedExcetion depends on our context. If the interrupteEx-
  pection will bubble up and eventually close our zk handle, we can let it go up
  the stack and everything will get cleaned up when the handle is closed. If the
  zk handle is not closed, we need to figure out if we are the master before re-
  throwing the exception or asynchronously continuing the operation. This latter 
  cause is particularly tricky and requires careful design to handle properly.
  
 # ZooKeeper Asynchronously request / callback
 
     The asynchronous method simpley queues the request to the ZooKeeper server.
   Transmission happens on another thread. When responses are recevied, they are
   processed on a dedicated callback thread. To preserve order, there a single 
   callback thread and responses are processed in the order they are recevied.
     Because a single thread process all callbacks, if a callback block, it blocks
   all callbacks that follow it. This means that generally you should not do 
   intensive operations or blocking operations in a callback. There may be times
   when it's legitimate to use the synchronous API in a callback, but it should
   generaly be avoided so that subsequent callbacks can be processed quickly.
   
# One-Time Triggers
      A watch is a one-time trigger associated with a znode and a type of event(e.g.
    , data is set in the znode, or the znode is deleted). When the watch the watch
    is triggered by an event, it generates a nitification. A notification is a 
    message to the application client that registered the watch to inform this 
    client of the event.
      When an application process regiesters a watch to recevie a notification,
    the watch is triggered at most once and upon the first event that matches the 
    condition of the watch. For example, say that the client needs to know when a 
    given znode /z is deleted(e.g., a backup master). The client executes an ex-
    ists operation on /z with the watch flag set and waits for the notification.
    The notification comes in the form of a callback to the application client.
      Each watch is associated with the session in which the client sets it. If
    the session expires, pending watches are removed. Watches do, however, persist
    across connections to different servers. Say that a ZooKeeper client disconnects
    from a ZooKeeper server and connects to a different server in the ensemble.
    The client will send a list outstanding watches. When reregiestering the 
    watch, the server will check to see if the watched znode has changed since
    the watch was regiested. If the znode has changed, a watch event will be sent
    to the client; otherwise, the watch will be reregiestered at the new server.
   
# Miss Events with One-time triggers

    An application can miss events between receiving a notification and register-
    ing for another watch. However, this issue deserves more discussion. Missing
    events is typically not a problem because any changes that have occurred dur-
    ing the period between receiving a notification and regiestering a new watch
    can be captured by reading the state of ZooKeeper directly.
    
# Watch Interface

    public void process(WatchEvent event);
    
    - The state of the ZooKeeper session(KeeperState):
      ```
        Disconnected
        SyncConnected
        AuthFailed
        ConnectedReadOnly
        SaslAuthenticated
        Expired
      ```
      
     - The event type(EventType)
       ```
         NodeCreated
         NodeDeleted
         NodeDataChanged
         NodeChildrenChanged
         None
       ```
       
     - A znode path in the case the watched event is not None
     
     - One important observation about watches is that currently it is not poss-
     ible to remove them once set. The only two to remove a watch are to have it
     triggered or for its session to be cloused or expired. This behavior is li-
     kely to change in future versions, however, because the community has been
     working on it for version 3.5.0.
     
# Multiop

    Multiop was not in the original design of ZooKeeper, but was added in version
  3.4.0. Multiop enables the execution of multiple ZooKeeper operations in a  
  block atomically. The execution is atomic in the sense that either all opera-
  tions in a multiop block succeed or all fail. For example, we can delete a pa-
  rent zonde and its child in a multiop block. The only possible outcomes are 
  that either both operations successd or both fail. It is not possible for the
  parent to be deleted while leaving one of its hcildren around, or vice versa.
    Multiop can simplify our master-worker implementation in at least one plcae.
  When assiging a task, the master in previous examples has created the corres-
  ponding assignment znode and the deleted the task znode under /tasks. if the 
  master crashes before deleting the znode under /tasks, we are left with a task
  in /tasks that has already been assigned. Using multiop, we can create the 
  znode representing the assignment of the task under /assign and delete the 
  zonde representing the task unser /tasks atomically. Using this approach, we
  guarantee that no task znode under /tasks has been already assigned. If a 
  backup takes over the role of master, is is not necessary to disambiguate the
  tasks in /tasks: they are all unassigned.
    Another feature that multiop offers is the possibility of checking the 
  version of a znode to enable operations over multiple zondes that read the state
  of ZooKeeper and write back some data-possibly a modification of what has been
  read. The version of the znode that is checked does not change, so this call
  enables a multiop that checks the version of z znode that is not modified. This
  feature is useful when the changes to one or more znodes are conditional upon 
  the version of another znode. Say that in our master-worker example, the master
  needs to have the clients adding new tasks under a path that the master speifies.
  For example, the master could ask clients to create new task as children of 
  /tasks-mid, where mid is the master identifier. The master stores this path
  as the data of the /master-path znode. A client that needs to add a new task
  first reads /master-pth and picks its current version with Stat. Next, the client
  creates a new task zonde under /task-mid as part of the a multiop call. and it
  also checks that the version of /master-path matches the one it has read.
  
# Recoverable and Unrecoverable Failure

    Rather than trying to determine causes of failures, ZooKeeper expose two cla-
  sses of failure: recoverable and unrecoverable. Recoverable failures are trans-
  ient and should be considered relatively normal-things happen. Brief network 
  hiccups and server failures can cause these kinds of failures. Developers should
  write theire code so that theire applications keep running in spite of these
  failures.
    Unrecoverable failure are much more problematic. These kinds of failures cause
  the ZooKeeper handle to become inoperable. The easiest and most common way to 
  deal with this kind of failure is to exit the application. Examples of causes 
  of this class of failure are session timeouts, networks outages for longer than
  the session timeout, and authentication failures.
  
  
# The Exist Watch and the Disconnected Event

    To make session disconnection and reestablishment a little more seamless, the
  ZooKeeper client library will reestablish any existing watches on the new server.
  When the client library connects to a ZooKeeper server, it will send the list of 
  outstanding watches and the last zxid(the last state tiemstamp) it has been. 
  The server will go through the watches and check the modification timestamps of
  the znodes that correspond to them. If any watched znodes have a modification
  timestamp later than the last zxid seen, the server will trigger the watch.
    This logic works perfectly for every ZooKeeper operation except exist. The
  exists operation is different from all other operatons because it can set a 
  watch on a znode that does not exist. If we look closely at the watch registra-
  tion logic in the previous paragraph, we are that there is a corner case in 
  which we can miss a watch event.
    Figure 5-5 illustrates the corner case that cause us to miss the creation 
   event of a watched znode. In this example, the client is watching for the
   creation of /event. However, just as the /event is created by another client,
   the watching client loses its connection to ZooKeeper. During this time the 
   other client deletes /event, so when the watching client reconnection to Zoo-
   Keeper and reregisters its watch, the ZooKeeper erver no longer has the /event
   znode. Thus, when it processes the regiestered watches and sees the watch for
   /event, and sees that there is no node called /event, it simply reregiesters 
   the watch, causing the client to miss the creation event for /event. Because
   of this corner case, you should try to avoid watching for the creation event
   of a znode. If you do watch for a creation event it should be for a long-lived
   znode; otherwise, this corner case can bite you.
   
   
#  Fencing Token

     Another approach is to extend the coordination data provided by ZooKeeper to
   the external device, using a technique call fencing. This is used often in 
   distributed systems to ensure exclusive access to a resource.
     We will show an example of implementng simple fencing using a fencing token.
   As long as a client holds the most recent token, it can access the resource.
     When we create a leaser znode, we get back a Stat structure. One of the 
   members of that structures it the czxid, which is the zxid that created the 
   znode. The zxid is a unique, monotonically increasing sequence number. We can
   use the czxid as a fencing token.
     When we make a request to the external resource, or when connect to the 
   external resource, we also supply the fencing token. If the external resource
   has recevied a request or connection with a higher fencing token, our request
   or connecton will be rejected. This means that once a new master connects to 
   an external resource and starts managing it, if an older master tries to do 
   anything which the external resource, its request wiil fail; it will be fenced
   off. Fencing has the nice benefit that it will work reliably event in the pre-
   sence of system overload or clock drift. 
     Figure 5-7 shows how this technique solves the scenario of Figure 5-6. When
   c1 becomes the leader at time t1, the creation zxid of /leader znode is 3(in
   reality, the zxid would be a much larger number). It supplies the creation 
   zxid as the fencing token to connect with the database. Later, when c1 becomes
   unresponsive due to overload, ZooKeeper declares c1 as failed and c2 becomes 
   the new leader at time t2. c2 uses 4 as its fencing token because the /leader
   zonde it created has a creation zxid of 4. At time t3, c3 starts making requests
   to the database using its fencing token. Now weh c1's request arrives at the
   datbase at time t4, it is rejected because its fencing token(3) is lower than
   the highest seen fencing tokne(4), thus avoiding corruption.
     Unfortunately, this fencing scheme requires changes to the protocol between
   the client and the resource. There must be room in the protocol to add the 
   zxid, and the external resource needs a persistent store to track the latest
   zxid received.
   
# Order in the presence of connection loss
    Upon a connection loss event, ZooKeeper cancels pending request. For synch-
  ronous cals the library throws an exception, while for asynchronous call the 
  library invokes the callbacks with a return code indicating a connection loss.
  Thie client library won't try to resubmit hte request once it has indicated to
  the applicaiton that the connection has been lost, so it's up to the application
  to resubmit operations that have been canceled. Consequently, the application 
  can repy on the client library to isue al the callbacks, but it cannot rely on
  ZooKeeper to actually carry out the operation in the event of a connection loss.
    To understand what impat this can have on an application, let's consider the
  following sequence of events:
    - Applicaiotn submits a request to execute op1
    - Client library detects a connection loss and cancels pending request to 
      execute op1
    - Client reconnections before the session expires
    - Op2 is executed successfully
    - Op1 returns with CONNETIONLOSS
    - Appliction resubmits op1
     In this case, the application submitted op1 and op2 in that order and got 
  op2 to execute successfully before op1. When the application notices the con-
  nection loss in the callback of op1, it tries to submit that request again. But
  suppose the client does not successfully reconnect. A new call to submit op1
  will report connection loss again, and there is a risk that the application 
  will enter into an unbounded loop of resubmitting the op1 request without con-
  necting. To get out of this cycle, the applicaiton could set a bound on the 
  number of attempts, or close the handle if reconnecting takes too long.
    In some cases, it might be important to make sure that op1 is executed 
  successfully before op2. if op2 depends on op1 in some way, then to avoid hav-
  ing op2 executing successfully before op1, we could simply wait for a success-
  ful execution of op1 before submitting op2. This is the approach we have taken
  in most of our master-worker example code, to guarantee that the request are
  executed in order. In general, this approach of waiting ofr the result of op1
  is safe, but it adds a performance penalty because the application needs to 
  wait for the result of one request to submit the next one, instead of having
  them processed concurrently.

# Order with the synchronous API and multiply thread

    Multithreaded applications are common these days. If you are using the syn-
  chronous API with multiply threads, it is important to pay attention to one
  ordering issue. A synchronous ZooKeeper call blocks until it gets a response.
  If two or more threads submit synchronous operations to ZooKeeper concurrently,
  they will each block until they receive a response. ZooKeeper will deliver the 
  responses in order, but it is possible that due to thread scheduling, the result
  of an operation submitted later will be processed first. If ZooKeeper delivers
  responses to operations very close to each other in time, you may observe such
  a scenario.
    If different threads submit operations concurrently, odds are that the oper-
  ations are not directly related and can be executed in any order without 
  causing consistency issue.
    But if the operations are related, the application client must take the order
  of submission into consideration when procesing the results.    

# Order when mixing synchronous and asychronous calls
    Here's another situation where results may appear to be out of order. Say that
  you submit two asynchronous operation, aop1 and aop2. It doesn't matter what
  the operations are, only that they are asynchronous. In the callback of aop1,
  you make a synchronous call, sop1. The synchronous call blocks the dispatch
  thread of the ZooKeeper client, which causes the application client to recevie
  the result of sop1 before reveiving the result of aop2. Consequently, the 
  application observers the results of aop1, sop1, and aop2 in that order, which
  is different from the submission order.
    In general, it is not a good idea to mix synchronous and asychronous calls.
  There are a few exceptions - for example, when starting up, you may want to 
  make sure that some data is ZooKeeper before proceeding. It is possible to use
  Java latches and order such mechanisms in these case, but one or more 
  synchronous calls might also do the job.
  
# Data and child limits

    ZooKeeper limits the amount of data transferred for eache request to 1MB by
  default. This limit bounds the maximum amount of data for any given node and 
  the number of children any parent znode can have. The choice of 1MB is 
  somewhat arbitrary, in the sense that there is noting fundamental in ZooKeeper
  that prevents it from usng a different value, larger or smaller. A limit 
  exists, however, to keep performance high. A znode with very large data, for
  instance, takes a long time to traverse, essentially stalling the processing
  pipeline while requests are being executed. The same problem happens if a 
  client executes getChildren on a znode with a large number of children.
    The limits ZooKeeper imposes right now for data size and number of children
  are already quite high, so you should avoid event getting close to these 
  limits. We have enabled large limits to satify the requirements of a broader 
  set of applications. If you have a special use case and realyy need different
  limit, you can change them.
  
  
# references
 
  - ZooKeeper Distributed Process Coordination.O'Reilly.2013